# Agent æ¡†æ¶é€‰æ‹©æŒ‡å—

## ä¸€ã€æ¡†æ¶æ¨èï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### ğŸ¥‡ ç¬¬ä¸€æ¢¯é˜Ÿï¼šä¸“ä¸š Agent æ¡†æ¶

#### 1. **LangGraph** (LangChain) â­â­â­â­â­
- **å®šä½**ï¼šæ„å»ºæœ‰çŠ¶æ€çš„å¤š Agent åº”ç”¨
- **ä¼˜åŠ¿**ï¼š
  - ğŸ”„ **çŠ¶æ€ç®¡ç†**ï¼šå†…ç½®çŠ¶æ€æœºï¼Œå®Œç¾é€‚åˆå‰§æƒ…æ¨è¿›
  - ğŸ“Š **å¯è§†åŒ–**ï¼šæµç¨‹å›¾å¯è§†åŒ–ï¼Œæ˜“äºè°ƒè¯•
  - ğŸ”§ **çµæ´»æ€§**ï¼šå¯ä»¥å®šä¹‰å¤æ‚çš„å·¥ä½œæµ
  - ğŸ§© **ç»„ä»¶ä¸°å¯Œ**ï¼šä¸ LangChain ç”Ÿæ€é›†æˆ
  - ğŸ“š **æ–‡æ¡£å®Œå–„**ï¼šå¤§é‡ç¤ºä¾‹å’Œæ•™ç¨‹
- **åŠ£åŠ¿**ï¼š
  - ğŸ“ˆ å­¦ä¹ æ›²çº¿ç¨é™¡
  - ğŸ ä»…æ”¯æŒ Python
- **é€‚ç”¨åœºæ™¯**ï¼šå¤æ‚å‰§æƒ…ã€å¤šæ­¥éª¤æ¨ç†ã€éœ€è¦çŠ¶æ€ç®¡ç†

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
import operator

# å®šä¹‰çŠ¶æ€
class StoryState(TypedDict):
    chapter: int
    situation: str
    variables: dict
    messages: Annotated[list, operator.add]
    decision: dict

# åˆ›å»ºå›¾
workflow = StateGraph(StoryState)

# å®šä¹‰èŠ‚ç‚¹
def get_ink_state(state):
    """è·å– ink çŠ¶æ€"""
    ink_state = call_ink_engine(state)
    return {"ink_state": ink_state}

def retrieve_memories(state):
    """æ£€ç´¢è®°å¿†"""
    memories = search_vector_db(state["messages"][-1])
    return {"memories": memories}

def generate_story(state):
    """ç”Ÿæˆå‰§æƒ…"""
    response = call_llm(state)
    return {"messages": [response]}

def judge_situation(state):
    """åˆ¤å®šæ¨è¿›"""
    decision = evaluate_progress(state)
    return {"decision": decision}

def update_state(state):
    """æ›´æ–°çŠ¶æ€"""
    if state["decision"]["shouldAdvanceChapter"]:
        return {"chapter": state["chapter"] + 1}
    return {}

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("ink", get_ink_state)
workflow.add_node("memory", retrieve_memories)
workflow.add_node("generate", generate_story)
workflow.add_node("judge", judge_situation)
workflow.add_node("update", update_state)

# å®šä¹‰è¾¹
workflow.set_entry_point("ink")
workflow.add_edge("ink", "memory")
workflow.add_edge("memory", "generate")
workflow.add_edge("generate", "judge")

# æ¡ä»¶åˆ†æ”¯
def should_continue(state):
    if state["decision"]["shouldEndStory"]:
        return "end"
    return "update"

workflow.add_conditional_edges(
    "judge",
    should_continue,
    {
        "update": "update",
        "end": END
    }
)
workflow.add_edge("update", END)

# ç¼–è¯‘å¹¶è¿è¡Œ
app = workflow.compile()
result = app.invoke({
    "chapter": 1,
    "situation": "initial",
    "variables": {},
    "messages": [{"role": "user", "content": "æˆ‘è¦æ•´é¡¿åæ²»"}]
})
```

**ä¸ºä»€ä¹ˆæ¨è**ï¼š
- âœ… å®Œç¾é€‚åˆæ•…äº‹æ¨è¿›çš„çŠ¶æ€æœºæ¨¡å‹
- âœ… å¯ä»¥æ¸…æ™°å®šä¹‰ï¼šink â†’ è®°å¿† â†’ LLM â†’ åˆ¤å®š â†’ æ›´æ–°
- âœ… æ”¯æŒå¾ªç¯å’Œæ¡ä»¶åˆ†æ”¯
- âœ… å†…ç½®æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥æš‚åœå’Œæ¢å¤

#### 2. **CrewAI** â­â­â­â­
- **å®šä½**ï¼šå¤š Agent åä½œæ¡†æ¶
- **ä¼˜åŠ¿**ï¼š
  - ğŸ‘¥ **å¤š Agent**ï¼šå¯ä»¥å®šä¹‰å¤šä¸ªè§’è‰²ï¼ˆå™äº‹è€…ã€åˆ¤å®šè€…ã€è®°å¿†ç®¡ç†è€…ï¼‰
  - ğŸ­ **è§’è‰²æ‰®æ¼”**ï¼šæ¯ä¸ª Agent æœ‰è‡ªå·±çš„è§’è‰²å’Œç›®æ ‡
  - ğŸ”§ **ç®€å•æ˜“ç”¨**ï¼šé…ç½®å¼å¼€å‘ï¼Œä»£ç é‡å°‘
  - ğŸš€ **å¿«é€Ÿä¸Šæ‰‹**ï¼šå­¦ä¹ æ›²çº¿å¹³ç¼“
- **åŠ£åŠ¿**ï¼š
  - ğŸ”’ çµæ´»æ€§ä¸å¦‚ LangGraph
  - ğŸ“Š çŠ¶æ€ç®¡ç†è¾ƒå¼±
- **é€‚ç”¨åœºæ™¯**ï¼šå¤šè§’è‰²åä½œã€ç®€å•å·¥ä½œæµ

```python
from crewai import Agent, Task, Crew

# å®šä¹‰ Agent
narrator = Agent(
    role='æ•…äº‹å™äº‹è€…',
    goal='æ ¹æ®ç©å®¶é€‰æ‹©ç”Ÿæˆç”ŸåŠ¨çš„å‰§æƒ…',
    backstory='ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„å†å²å°è¯´å®¶',
    llm='claude-3-5-sonnet'
)

judge = Agent(
    role='å‰§æƒ…åˆ¤å®šè€…',
    goal='è¯„ä¼°å‰§æƒ…è¿›å±•ï¼Œå†³å®šæ˜¯å¦æ¨è¿›ç« èŠ‚',
    backstory='ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„æ¸¸æˆè®¾è®¡å¸ˆ',
    llm='gpt-4o-mini'
)

memory_keeper = Agent(
    role='è®°å¿†ç®¡ç†è€…',
    goal='æ£€ç´¢å’Œç®¡ç†å†å²äº‹ä»¶',
    backstory='ä½ è´Ÿè´£ç»´æŠ¤æ•…äº‹çš„è¿è´¯æ€§',
    llm='gpt-4o-mini'
)

# å®šä¹‰ä»»åŠ¡
narrate_task = Task(
    description='æ ¹æ®ç©å®¶è¾“å…¥"{user_input}"ç”Ÿæˆå‰§æƒ…',
    agent=narrator
)

judge_task = Task(
    description='è¯„ä¼°å‰§æƒ…è¿›å±•ï¼Œåˆ¤æ–­æ˜¯å¦æ¨è¿›',
    agent=judge,
    context=[narrate_task]  # ä¾èµ–å™äº‹ä»»åŠ¡
)

# åˆ›å»º Crew
crew = Crew(
    agents=[narrator, judge, memory_keeper],
    tasks=[narrate_task, judge_task],
    verbose=True
)

# æ‰§è¡Œ
result = crew.kickoff(inputs={"user_input": "æˆ‘è¦æ•´é¡¿åæ²»"})
```

**ä¸ºä»€ä¹ˆæ¨è**ï¼š
- âœ… é€‚åˆå°† Agent åˆ†è§£ä¸ºå¤šä¸ªè§’è‰²
- âœ… ä»£ç ç®€æ´ï¼Œæ˜“äºç»´æŠ¤
- âœ… è‡ªåŠ¨å¤„ç† Agent é—´é€šä¿¡

#### 3. **AutoGen** (Microsoft) â­â­â­â­
- **å®šä½**ï¼šå¤š Agent å¯¹è¯æ¡†æ¶
- **ä¼˜åŠ¿**ï¼š
  - ğŸ’¬ **å¯¹è¯å¼**ï¼šAgent ä¹‹é—´å¯ä»¥å¯¹è¯
  - ğŸ”§ **å·¥å…·è°ƒç”¨**ï¼šæ”¯æŒ Function Calling
  - ğŸ¢ **ä¼ä¸šçº§**ï¼šMicrosoft æ”¯æŒ
  - ğŸ“š **æ–‡æ¡£å®Œå–„**
- **åŠ£åŠ¿**ï¼š
  - ğŸ¯ æ›´é€‚åˆå¯¹è¯åœºæ™¯ï¼Œä¸å¤ªé€‚åˆçŠ¶æ€æœº
  - ğŸŒ æ€§èƒ½ä¸€èˆ¬
- **é€‚ç”¨åœºæ™¯**ï¼šå¤š Agent è®¨è®ºã€å¤æ‚å†³ç­–

```python
from autogen import AssistantAgent, UserProxyAgent

# å®šä¹‰ Agent
narrator = AssistantAgent(
    name="narrator",
    system_message="ä½ æ˜¯æ•…äº‹å™äº‹è€…",
    llm_config={"model": "claude-3-5-sonnet"}
)

judge = AssistantAgent(
    name="judge",
    system_message="ä½ æ˜¯å‰§æƒ…åˆ¤å®šè€…",
    llm_config={"model": "gpt-4o-mini"}
)

# Agent å¯¹è¯
narrator.initiate_chat(
    judge,
    message="ç©å®¶é€‰æ‹©äº†æ•´é¡¿åæ²»ï¼Œæˆ‘ç”Ÿæˆäº†ä»¥ä¸‹å‰§æƒ…ï¼š..."
)
```

### ğŸ¥ˆ ç¬¬äºŒæ¢¯é˜Ÿï¼šè½»é‡çº§æ¡†æ¶

#### 4. **LangChain** (ä¸ä½¿ç”¨ LangGraph) â­â­â­
- **å®šä½**ï¼šLLM åº”ç”¨å¼€å‘æ¡†æ¶
- **ä¼˜åŠ¿**ï¼š
  - ğŸ§© **ç»„ä»¶ä¸°å¯Œ**ï¼šChains, Memory, Tools
  - ğŸŒ **ç”Ÿæ€å®Œå–„**ï¼šå¤§é‡é›†æˆ
  - ğŸ“š **ç¤¾åŒºæ´»è·ƒ**
- **åŠ£åŠ¿**ï¼š
  - ğŸ”„ çŠ¶æ€ç®¡ç†è¾ƒå¼±ï¼ˆéœ€è¦ LangGraphï¼‰
  - ğŸ“ˆ å¤æ‚åº¦é«˜
- **é€‚ç”¨åœºæ™¯**ï¼šç®€å• Agentã€å¿«é€ŸåŸå‹

```python
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-5-sonnet")
memory = ConversationBufferMemory()

chain = LLMChain(
    llm=llm,
    memory=memory,
    prompt=story_prompt
)

response = chain.run(user_input="æˆ‘è¦æ•´é¡¿åæ²»")
```

#### 5. **Semantic Kernel** (Microsoft) â­â­â­
- **å®šä½**ï¼šè·¨è¯­è¨€ AI ç¼–æ’æ¡†æ¶
- **ä¼˜åŠ¿**ï¼š
  - ğŸŒ **å¤šè¯­è¨€**ï¼šPython, C#, Java
  - ğŸ”§ **æ’ä»¶ç³»ç»Ÿ**ï¼šæ˜“äºæ‰©å±•
  - ğŸ¢ **ä¼ä¸šçº§**
- **åŠ£åŠ¿**ï¼š
  - ğŸ“š æ–‡æ¡£è¾ƒå°‘
  - ğŸ› ç›¸å¯¹ä¸æˆç†Ÿ
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å¤šè¯­è¨€æ”¯æŒ

#### 6. **Haystack** â­â­â­
- **å®šä½**ï¼šNLP å’Œ RAG æ¡†æ¶
- **ä¼˜åŠ¿**ï¼š
  - ğŸ” **RAG ä¼˜ç§€**ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆ
  - ğŸ“Š **Pipeline**ï¼šæ¸…æ™°çš„ç®¡é“ç»“æ„
- **åŠ£åŠ¿**ï¼š
  - ğŸ¯ æ›´é€‚åˆ RAGï¼Œä¸å¤ªé€‚åˆ Agent
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å¤§é‡æ£€ç´¢çš„åœºæ™¯

### ğŸ¥‰ ç¬¬ä¸‰æ¢¯é˜Ÿï¼šè‡ªå»ºæ–¹æ¡ˆ

#### 7. **çº¯ FastAPI + Celery** â­â­â­â­
- **å®šä½**ï¼šå®Œå…¨è‡ªå®šä¹‰
- **ä¼˜åŠ¿**ï¼š
  - ğŸ”§ **å®Œå…¨æ§åˆ¶**ï¼šæ²¡æœ‰æ¡†æ¶é™åˆ¶
  - âš¡ **æ€§èƒ½æœ€ä¼˜**ï¼šæ²¡æœ‰é¢å¤–å¼€é”€
  - ğŸ“¦ **è½»é‡çº§**ï¼šåªç”¨éœ€è¦çš„ç»„ä»¶
- **åŠ£åŠ¿**ï¼š
  - ğŸ› ï¸ éœ€è¦è‡ªå·±å®ç°æ‰€æœ‰é€»è¾‘
  - ğŸ“ˆ å¼€å‘æ—¶é—´é•¿
- **é€‚ç”¨åœºæ™¯**ï¼šæœ‰æ˜ç¡®éœ€æ±‚ã€è¿½æ±‚æè‡´æ€§èƒ½

```python
# å°±æ˜¯æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ agent-server/main.py
# å®Œå…¨è‡ªå®šä¹‰ï¼Œçµæ´»æ€§æœ€é«˜
```

## äºŒã€é’ˆå¯¹æ•…äº‹æ¨è¿›åœºæ™¯çš„æ¨è

### ğŸ¯ æœ€ä½³é€‰æ‹©ï¼šLangGraph

**ç†ç”±**ï¼š
1. âœ… **çŠ¶æ€æœºæ¨¡å‹**ï¼šå®Œç¾åŒ¹é…å‰§æƒ…æ¨è¿›é€»è¾‘
2. âœ… **å¯è§†åŒ–æµç¨‹**ï¼šæ˜“äºç†è§£å’Œè°ƒè¯•
3. âœ… **æ£€æŸ¥ç‚¹ç³»ç»Ÿ**ï¼šå¯ä»¥ä¿å­˜å’Œæ¢å¤æ¸¸æˆè¿›åº¦
4. âœ… **æ¡ä»¶åˆ†æ”¯**ï¼šæ”¯æŒå¤æ‚çš„å‰§æƒ…åˆ†æ”¯
5. âœ… **å¾ªç¯æ”¯æŒ**ï¼šå¯ä»¥åœ¨ç« èŠ‚å†…å¾ªç¯

### å®Œæ•´ç¤ºä¾‹ï¼šä½¿ç”¨ LangGraph æ„å»ºæ•…äº‹ Agent

```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.sqlite import SqliteSaver
from typing import TypedDict, Annotated, Literal
import operator

# ============ 1. å®šä¹‰çŠ¶æ€ ============
class StoryState(TypedDict):
    # åŸºç¡€ä¿¡æ¯
    session_id: str
    story_id: str
    user_input: str
    
    # å‰§æƒ…çŠ¶æ€
    chapter: int
    situation: str
    state_variables: dict
    
    # å¯¹è¯å†å²
    messages: Annotated[list, operator.add]
    
    # ä¸­é—´ç»“æœ
    ink_state: dict
    memories: list
    llm_response: str
    decision: dict
    
    # æ§åˆ¶æµ
    next_action: Literal["continue", "advance_chapter", "end_story"]

# ============ 2. å®šä¹‰èŠ‚ç‚¹å‡½æ•° ============

def load_session(state: StoryState) -> dict:
    """åŠ è½½ä¼šè¯çŠ¶æ€"""
    from database import get_session
    session = get_session(state["session_id"])
    return {
        "chapter": session["chapter"],
        "situation": session["situation"],
        "state_variables": session["state_variables"],
        "messages": session["messages"][-10:]  # æœ€è¿‘10æ¡
    }

def get_ink_state(state: StoryState) -> dict:
    """è°ƒç”¨ ink å¼•æ“"""
    from ink_engine import InkEngine
    engine = InkEngine(state["story_id"])
    ink_state = engine.get_state(
        chapter=state["chapter"],
        variables=state["state_variables"]
    )
    return {"ink_state": ink_state}

def retrieve_memories(state: StoryState) -> dict:
    """æ£€ç´¢ç›¸å…³è®°å¿†"""
    from vector_db import search_memories
    memories = search_memories(
        session_id=state["session_id"],
        query=state["user_input"],
        limit=5
    )
    return {"memories": memories}

def generate_story(state: StoryState) -> dict:
    """è°ƒç”¨ LLM ç”Ÿæˆå‰§æƒ…"""
    from anthropic import Anthropic
    
    client = Anthropic()
    
    # æ„å»ºæç¤ºè¯
    system_prompt = f"""
ä½ æ˜¯{state['story_id']}å‰§æœ¬çš„å™äº‹è€…ã€‚

å½“å‰ç« èŠ‚ï¼šç¬¬{state['chapter']}ç« 
å½“å‰å±€åŠ¿ï¼š{state['situation']}
çŠ¶æ€å˜é‡ï¼š{state['state_variables']}

å†å²è®°å¿†ï¼š
{chr(10).join([m['summary'] for m in state['memories']])}

è¯·æ ¹æ®ç©å®¶çš„é€‰æ‹©ç”Ÿæˆç”ŸåŠ¨çš„å‰§æƒ…ã€‚
"""
    
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[
            {"role": "system", "content": system_prompt},
            *state["messages"],
            {"role": "user", "content": state["user_input"]}
        ]
    )
    
    llm_response = response.content[0].text
    
    return {
        "llm_response": llm_response,
        "messages": [{"role": "assistant", "content": llm_response}]
    }

def judge_situation(state: StoryState) -> dict:
    """åˆ¤å®šå‰§æƒ…è¿›å±•"""
    from openai import OpenAI
    
    client = OpenAI()
    
    judgment_prompt = f"""
åˆ¤æ–­å‰§æƒ…è¿›å±•ï¼š

å½“å‰ç« èŠ‚ï¼š{state['chapter']}
çŠ¶æ€å˜é‡ï¼š{state['state_variables']}
æœ€æ–°å‰§æƒ…ï¼š{state['llm_response']}

è¿”å› JSONï¼š
{{
  "situationScore": 0-100,
  "shouldAdvanceChapter": true/false,
  "shouldEndStory": true/false,
  "rationale": "ç†ç”±",
  "stateChanges": {{"variable": value}}
}}
"""
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": judgment_prompt}],
        response_format={"type": "json_object"}
    )
    
    decision = json.loads(response.choices[0].message.content)
    
    # å†³å®šä¸‹ä¸€æ­¥
    if decision["shouldEndStory"]:
        next_action = "end_story"
    elif decision["shouldAdvanceChapter"]:
        next_action = "advance_chapter"
    else:
        next_action = "continue"
    
    return {
        "decision": decision,
        "next_action": next_action
    }

def update_database(state: StoryState) -> dict:
    """æ›´æ–°æ•°æ®åº“"""
    from database import save_message, update_session
    
    # ä¿å­˜æ¶ˆæ¯
    save_message(
        session_id=state["session_id"],
        role="user",
        content=state["user_input"]
    )
    save_message(
        session_id=state["session_id"],
        role="assistant",
        content=state["llm_response"]
    )
    
    # æ›´æ–°ä¼šè¯çŠ¶æ€
    new_variables = {
        **state["state_variables"],
        **state["decision"].get("stateChanges", {})
    }
    
    new_chapter = state["chapter"]
    if state["next_action"] == "advance_chapter":
        new_chapter += 1
    
    update_session(
        session_id=state["session_id"],
        chapter=new_chapter,
        situation=state["situation"],
        state_variables=new_variables
    )
    
    return {
        "chapter": new_chapter,
        "state_variables": new_variables
    }

def end_story(state: StoryState) -> dict:
    """ç»“æŸæ•…äº‹"""
    from database import mark_story_completed
    mark_story_completed(state["session_id"])
    return {"next_action": "end_story"}

# ============ 3. æ„å»ºå›¾ ============

def create_story_graph():
    workflow = StateGraph(StoryState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("load", load_session)
    workflow.add_node("ink", get_ink_state)
    workflow.add_node("memory", retrieve_memories)
    workflow.add_node("generate", generate_story)
    workflow.add_node("judge", judge_situation)
    workflow.add_node("update", update_database)
    workflow.add_node("end", end_story)
    
    # å®šä¹‰æµç¨‹
    workflow.set_entry_point("load")
    workflow.add_edge("load", "ink")
    workflow.add_edge("ink", "memory")
    workflow.add_edge("memory", "generate")
    workflow.add_edge("generate", "judge")
    
    # æ¡ä»¶åˆ†æ”¯
    def route_after_judge(state: StoryState) -> str:
        if state["next_action"] == "end_story":
            return "end"
        else:
            return "update"
    
    workflow.add_conditional_edges(
        "judge",
        route_after_judge,
        {
            "update": "update",
            "end": "end"
        }
    )
    
    workflow.add_edge("update", END)
    workflow.add_edge("end", END)
    
    # æ·»åŠ æ£€æŸ¥ç‚¹ï¼ˆå¯ä»¥ä¿å­˜å’Œæ¢å¤ï¼‰
    memory = SqliteSaver.from_conn_string(":memory:")
    
    return workflow.compile(checkpointer=memory)

# ============ 4. ä½¿ç”¨ ============

# åˆ›å»ºå›¾
app = create_story_graph()

# è¿è¡Œ
config = {"configurable": {"thread_id": "session_123"}}
result = app.invoke(
    {
        "session_id": "session_123",
        "story_id": "chongzhen",
        "user_input": "æˆ‘è¦æ•´é¡¿åæ²»"
    },
    config=config
)

# å¯ä»¥æš‚åœå’Œæ¢å¤
# ä¸‹æ¬¡ç»§ç»­æ¸¸æˆæ—¶ï¼Œä½¿ç”¨ç›¸åŒçš„ thread_id å³å¯æ¢å¤çŠ¶æ€
```

## ä¸‰ã€æ¡†æ¶å¯¹æ¯”è¡¨

| æ¡†æ¶ | å­¦ä¹ æ›²çº¿ | çµæ´»æ€§ | çŠ¶æ€ç®¡ç† | å¤šAgent | æ€§èƒ½ | æ¨èåº¦ |
|------|---------|--------|---------|---------|------|--------|
| **LangGraph** | ä¸­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **CrewAI** | ä½ | â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| **AutoGen** | ä¸­ | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| **LangChain** | é«˜ | â­â­â­â­ | â­â­ | â­â­ | â­â­â­ | â­â­â­ |
| **çº¯ FastAPI** | ä½ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |

## å››ã€æœ€ç»ˆæ¨è

### ğŸ¯ æœ€ä½³æ–¹æ¡ˆï¼šLangGraph + FastAPI + Celery

```
FastAPI (Web æœåŠ¡å™¨)
    â†“
Celery (ä»»åŠ¡é˜Ÿåˆ—)
    â†“
LangGraph (Agent ç¼–æ’)
    â”œâ”€â”€ ink å¼•æ“
    â”œâ”€â”€ è®°å¿†ç³»ç»Ÿ
    â”œâ”€â”€ LLM (Claude/GPT)
    â””â”€â”€ åˆ¤å®šå™¨
```

**ç†ç”±**ï¼š
- âœ… FastAPIï¼šé«˜æ€§èƒ½ Web æœåŠ¡
- âœ… Celeryï¼šå¤„ç†é•¿æ—¶é—´ä»»åŠ¡
- âœ… LangGraphï¼šæ¸…æ™°çš„ Agent æµç¨‹
- âœ… æœ€ä½³å®è·µç»„åˆ

### ä»£ç ç»“æ„

```
agent-server/
â”œâ”€â”€ main.py              # FastAPI æœåŠ¡å™¨
â”œâ”€â”€ celery_app.py        # Celery é…ç½®
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ graph.py         # LangGraph å®šä¹‰
â”‚   â”œâ”€â”€ nodes.py         # èŠ‚ç‚¹å‡½æ•°
â”‚   â””â”€â”€ state.py         # çŠ¶æ€å®šä¹‰
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ink_engine.py    # ink å¼•æ“
â”‚   â”œâ”€â”€ memory.py        # è®°å¿†ç³»ç»Ÿ
â”‚   â””â”€â”€ llm.py           # LLM å®¢æˆ·ç«¯
â””â”€â”€ database/
    â””â”€â”€ models.py        # æ•°æ®åº“æ¨¡å‹
```

## äº”ã€å¿«é€Ÿå¼€å§‹

```bash
# å®‰è£…ä¾èµ–
pip install langgraph langchain-anthropic langchain-openai fastapi celery redis

# åˆ›å»º Agent
python create_agent.py

# å¯åŠ¨æœåŠ¡
uvicorn main:app --reload

# å¯åŠ¨ Worker
celery -A celery_app worker --loglevel=info
```

è¿™æ ·çš„æ¶æ„æ—¢æœ‰æ¡†æ¶çš„ä¾¿åˆ©æ€§ï¼Œåˆä¿æŒäº†çµæ´»æ€§å’Œæ€§èƒ½ï¼
