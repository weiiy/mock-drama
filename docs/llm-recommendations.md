# å¤§æ¨¡å‹é€‰æ‹©æŒ‡å— - æ•…äº‹æ¨è¿›åœºæ™¯

## ä¸€ã€æ¨¡å‹æ¨èï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### ğŸ¥‡ ç¬¬ä¸€æ¢¯é˜Ÿï¼šæœ€é€‚åˆæ•…äº‹åˆ›ä½œ

#### 1. **Claude 3.5 Sonnet** (Anthropic) â­â­â­â­â­
- **ä¼˜åŠ¿**ï¼š
  - ğŸ­ **æœ€å¼ºå™äº‹èƒ½åŠ›**ï¼šæ“…é•¿é•¿ç¯‡æ•…äº‹åˆ›ä½œï¼Œæ–‡ç¬”ä¼˜ç¾
  - ğŸ§  **ä¸Šä¸‹æ–‡ç†è§£**ï¼š200K tokensï¼Œè®°å¿†åŠ›å¼º
  - ğŸ¯ **è§’è‰²ä¸€è‡´æ€§**ï¼šèƒ½å¾ˆå¥½åœ°ä¿æŒè§’è‰²æ€§æ ¼
  - ğŸ“– **æƒ…èŠ‚è¿è´¯æ€§**ï¼šå‰§æƒ…æ¨è¿›è‡ªç„¶æµç•…
  - ğŸš€ **é€Ÿåº¦å¿«**ï¼šå“åº”é€Ÿåº¦å¿«ï¼Œé€‚åˆå®æ—¶äº’åŠ¨
- **åŠ£åŠ¿**ï¼š
  - ğŸ’° ä»·æ ¼è¾ƒé«˜ï¼ˆ$3/1M input tokens, $15/1M output tokensï¼‰
  - ğŸŒ éœ€è¦ç§‘å­¦ä¸Šç½‘ï¼ˆå›½å†…è®¿é—®å—é™ï¼‰
- **é€‚ç”¨åœºæ™¯**ï¼šé«˜è´¨é‡å‰§æœ¬ã€å¤æ‚å‰§æƒ…ã€è§’è‰²æ‰®æ¼”
- **API**ï¼šé€šè¿‡ Anthropic API æˆ– AWS Bedrock

```python
# ä½¿ç”¨ç¤ºä¾‹
import anthropic

client = anthropic.Anthropic(api_key="your-api-key")
message = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=2048,
    messages=[
        {"role": "user", "content": "ä½œä¸ºå´‡ç¥¯çš‡å¸..."}
    ]
)
```

#### 2. **GPT-4o** (OpenAI) â­â­â­â­â­
- **ä¼˜åŠ¿**ï¼š
  - ğŸ¨ **åˆ›æ„ä¸°å¯Œ**ï¼šæ•…äº‹ç”Ÿæˆå¤šæ ·åŒ–
  - ğŸ”§ **å·¥å…·è°ƒç”¨**ï¼šæ”¯æŒ Function Callingï¼Œé€‚åˆ Agent
  - ğŸ“š **çŸ¥è¯†å¹¿åš**ï¼šå†å²ã€æ–‡åŒ–çŸ¥è¯†ä¸°å¯Œ
  - ğŸŒ **ç”Ÿæ€å®Œå–„**ï¼šå¤§é‡å·¥å…·å’Œæ¡†æ¶æ”¯æŒ
- **åŠ£åŠ¿**ï¼š
  - ğŸ’° ä»·æ ¼ä¸­ç­‰ï¼ˆ$2.5/1M input, $10/1M outputï¼‰
  - ğŸ”’ å†…å®¹å®¡æ ¸è¾ƒä¸¥æ ¼ï¼ˆå¯èƒ½æ‹’ç»æŸäº›å‰§æƒ…ï¼‰
- **é€‚ç”¨åœºæ™¯**ï¼šé€šç”¨å‰§æœ¬ã€éœ€è¦å·¥å…·è°ƒç”¨çš„ Agent
- **API**ï¼šOpenAI API

```python
from openai import OpenAI

client = OpenAI(api_key="your-api-key")
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯å´‡ç¥¯çš‡å¸å‰§æœ¬çš„å™äº‹è€…"},
        {"role": "user", "content": "æˆ‘è¦æ•´é¡¿åæ²»"}
    ]
)
```

#### 3. **Gemini 1.5 Pro** (Google) â­â­â­â­
- **ä¼˜åŠ¿**ï¼š
  - ğŸ“– **è¶…é•¿ä¸Šä¸‹æ–‡**ï¼š1M tokensï¼ˆæœ€é•¿ï¼‰
  - ğŸ’° **æ€§ä»·æ¯”é«˜**ï¼šå…è´¹é¢åº¦å¤§
  - ğŸŒ **å›½å†…å¯è®¿é—®**ï¼šé€šè¿‡ Google Cloud äºšæ´²åŒº
  - ğŸ¯ **å¤šæ¨¡æ€**ï¼šæ”¯æŒå›¾ç‰‡ã€è§†é¢‘ï¼ˆæœªæ¥å¯æ‰©å±•ï¼‰
- **åŠ£åŠ¿**ï¼š
  - ğŸ“ æ–‡å­¦æ€§ç¨å¼±äº Claude
  - ğŸŒ å“åº”é€Ÿåº¦è¾ƒæ…¢
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦è¶…é•¿ä¸Šä¸‹æ–‡ã€æˆæœ¬æ•æ„Ÿé¡¹ç›®
- **API**ï¼šGoogle AI Studio / Vertex AI

```python
import google.generativeai as genai

genai.configure(api_key="your-api-key")
model = genai.GenerativeModel('gemini-1.5-pro')
response = model.generate_content("ä½œä¸ºå´‡ç¥¯çš‡å¸...")
```

### ğŸ¥ˆ ç¬¬äºŒæ¢¯é˜Ÿï¼šæ€§ä»·æ¯”ä¹‹é€‰

#### 4. **Claude 3 Haiku** (Anthropic) â­â­â­â­
- **ä¼˜åŠ¿**ï¼š
  - âš¡ **é€Ÿåº¦æœ€å¿«**ï¼šé€‚åˆå®æ—¶äº’åŠ¨
  - ğŸ’° **ä»·æ ¼ä¾¿å®œ**ï¼š$0.25/1M input, $1.25/1M output
  - ğŸ“– **è´¨é‡ä¸é”™**ï¼šè™½ä¸å¦‚ Sonnetï¼Œä½†è¶³å¤Ÿç”¨
- **é€‚ç”¨åœºæ™¯**ï¼šå¿«é€Ÿå“åº”ã€é«˜å¹¶å‘åœºæ™¯

#### 5. **DeepSeek V3** (å›½äº§) â­â­â­â­
- **ä¼˜åŠ¿**ï¼š
  - ğŸ‡¨ğŸ‡³ **å›½å†…å¯ç”¨**ï¼šæ— éœ€ç§‘å­¦ä¸Šç½‘
  - ğŸ’° **è¶…ä½ä»·æ ¼**ï¼š$0.27/1M input, $1.1/1M output
  - ğŸ­ **ä¸­æ–‡ä¼˜ç§€**ï¼šä¸­æ–‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›å¼º
  - ğŸ“š **çŸ¥è¯†ä¸°å¯Œ**ï¼šå°¤å…¶æ˜¯ä¸­å›½å†å²æ–‡åŒ–
- **åŠ£åŠ¿**ï¼š
  - ğŸ”’ å†…å®¹å®¡æ ¸ä¸¥æ ¼
  - ğŸ“ åˆ›æ„æ€§ç¨å¼±
- **é€‚ç”¨åœºæ™¯**ï¼šå›½å†…éƒ¨ç½²ã€ä¸­æ–‡å‰§æœ¬ã€æˆæœ¬æ•æ„Ÿ

```python
from openai import OpenAI

# DeepSeek å…¼å®¹ OpenAI API
client = OpenAI(
    api_key="your-deepseek-key",
    base_url="https://api.deepseek.com"
)
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[...]
)
```

#### 6. **Qwen2.5-72B** (é˜¿é‡Œé€šä¹‰åƒé—®) â­â­â­â­
- **ä¼˜åŠ¿**ï¼š
  - ğŸ‡¨ğŸ‡³ å›½å†…å¯ç”¨ï¼Œå“åº”å¿«
  - ğŸ’° ä»·æ ¼ä¾¿å®œ
  - ğŸ“– ä¸­æ–‡èƒ½åŠ›å¼º
  - ğŸ”“ å¼€æºå¯è‡ªéƒ¨ç½²
- **é€‚ç”¨åœºæ™¯**ï¼šå›½å†…é¡¹ç›®ã€å¯è‡ªéƒ¨ç½²

### ğŸ¥‰ ç¬¬ä¸‰æ¢¯é˜Ÿï¼šå¼€æºè‡ªéƒ¨ç½²

#### 7. **Llama 3.1 70B/405B** (Meta) â­â­â­
- **ä¼˜åŠ¿**ï¼š
  - ğŸ†“ **å®Œå…¨å…è´¹**ï¼šå¼€æºå¯å•†ç”¨
  - ğŸ”§ **å¯è‡ªéƒ¨ç½²**ï¼šæ•°æ®éšç§
  - ğŸŒ **ç¤¾åŒºæ´»è·ƒ**ï¼šå¤§é‡ä¼˜åŒ–ç‰ˆæœ¬
- **åŠ£åŠ¿**ï¼š
  - ğŸ’» éœ€è¦ GPU èµ„æº
  - ğŸ“ è´¨é‡ä¸å¦‚å•†ä¸šæ¨¡å‹
- **é€‚ç”¨åœºæ™¯**ï¼šæœ‰ GPU èµ„æºã€éœ€è¦æ•°æ®éšç§

#### 8. **Mistral Large** â­â­â­
- **ä¼˜åŠ¿**ï¼š
  - ğŸ‡ªğŸ‡º æ¬§æ´²æ¨¡å‹ï¼Œéšç§å‹å¥½
  - ğŸ’° ä»·æ ¼é€‚ä¸­
  - ğŸ“– å¤šè¯­è¨€èƒ½åŠ›å¼º
- **é€‚ç”¨åœºæ™¯**ï¼šæ¬§æ´²å¸‚åœºã€å¤šè¯­è¨€æ”¯æŒ

## äºŒã€é’ˆå¯¹ä¸åŒåœºæ™¯çš„æ¨è

### åœºæ™¯ 1ï¼šé«˜è´¨é‡å•äººå‰§æœ¬ï¼ˆå¦‚å´‡ç¥¯çš‡å¸ï¼‰
**æ¨è**ï¼šClaude 3.5 Sonnet
- æ–‡ç¬”ä¼˜ç¾ï¼Œå†å²æ„Ÿå¼º
- è§’è‰²ä¸€è‡´æ€§å¥½
- å‰§æƒ…æ¨è¿›è‡ªç„¶

### åœºæ™¯ 2ï¼šå¤šäººåœ¨çº¿ã€é«˜å¹¶å‘
**æ¨è**ï¼šClaude 3 Haiku + GPT-4o-mini
- é€Ÿåº¦å¿«ï¼Œæˆæœ¬ä½
- å¯ä»¥æ··ç”¨ï¼šHaiku ç”Ÿæˆå†…å®¹ï¼ŒGPT-4o-mini åšåˆ¤å®š

### åœºæ™¯ 3ï¼šå›½å†…éƒ¨ç½²ã€æˆæœ¬æ•æ„Ÿ
**æ¨è**ï¼šDeepSeek V3 + Qwen2.5
- å›½å†…å¯ç”¨ï¼Œæ— éœ€ç§‘å­¦ä¸Šç½‘
- ä»·æ ¼æä½
- ä¸­æ–‡èƒ½åŠ›å¼º

### åœºæ™¯ 4ï¼šéœ€è¦è¶…é•¿è®°å¿†
**æ¨è**ï¼šGemini 1.5 Pro
- 1M tokens ä¸Šä¸‹æ–‡
- å¯ä»¥è®°ä½æ•´ä¸ªæ¸¸æˆå†å²

### åœºæ™¯ 5ï¼šå®Œå…¨è‡ªä¸»å¯æ§
**æ¨è**ï¼šLlama 3.1 70Bï¼ˆè‡ªéƒ¨ç½²ï¼‰
- æ•°æ®éšç§
- æ—  API è°ƒç”¨æˆæœ¬
- éœ€è¦ GPU æœåŠ¡å™¨

## ä¸‰ã€æ··åˆç­–ç•¥ï¼ˆæ¨èï¼‰

### ç­–ç•¥ 1ï¼šåŒæ¨¡å‹æ¶æ„
```python
class HybridLLM:
    def __init__(self):
        self.creative_model = "claude-3-5-sonnet"  # ç”Ÿæˆå‰§æƒ…
        self.judge_model = "gpt-4o-mini"           # åˆ¤å®šæ¨è¿›
    
    async def generate_story(self, prompt):
        # ä½¿ç”¨ Claude ç”Ÿæˆé«˜è´¨é‡å‰§æƒ…
        return await call_claude(prompt)
    
    async def judge_situation(self, context):
        # ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹åšåˆ¤å®š
        return await call_gpt4o_mini(context)
```

**ä¼˜åŠ¿**ï¼š
- âœ… è´¨é‡ä¸æˆæœ¬å¹³è¡¡
- âœ… Claude ç”Ÿæˆå†…å®¹ï¼ˆ$15/1M outputï¼‰
- âœ… GPT-4o-mini åˆ¤å®šï¼ˆ$0.6/1M outputï¼‰
- âœ… æˆæœ¬é™ä½ 70%

### ç­–ç•¥ 2ï¼šåˆ†çº§è·¯ç”±
```python
class SmartRouter:
    def route_request(self, complexity):
        if complexity == "high":
            return "claude-3-5-sonnet"  # é‡è¦å‰§æƒ…
        elif complexity == "medium":
            return "gpt-4o"             # å¸¸è§„å¯¹è¯
        else:
            return "claude-3-haiku"     # ç®€å•å“åº”
```

### ç­–ç•¥ 3ï¼šç¼“å­˜ + æ¨¡å‹
```python
# å¸¸è§å¯¹è¯ä½¿ç”¨ç¼“å­˜
cache_hit = redis.get(f"response:{user_input_hash}")
if cache_hit:
    return cache_hit

# ç¼“å­˜æœªå‘½ä¸­æ‰è°ƒç”¨æ¨¡å‹
response = await call_llm(user_input)
redis.set(f"response:{user_input_hash}", response)
```

## å››ã€æˆæœ¬å¯¹æ¯”ï¼ˆ1000 æ¬¡å¯¹è¯ï¼‰

å‡è®¾æ¯æ¬¡å¯¹è¯ï¼š
- Input: 2000 tokensï¼ˆä¸Šä¸‹æ–‡ï¼‰
- Output: 500 tokensï¼ˆç”Ÿæˆå†…å®¹ï¼‰

| æ¨¡å‹ | Input æˆæœ¬ | Output æˆæœ¬ | æ€»æˆæœ¬ | è´¨é‡è¯„åˆ† |
|------|-----------|------------|--------|---------|
| Claude 3.5 Sonnet | $6 | $7.5 | **$13.5** | â­â­â­â­â­ |
| GPT-4o | $5 | $5 | **$10** | â­â­â­â­â­ |
| Gemini 1.5 Pro | $7 | $21 | **$28** | â­â­â­â­ |
| Claude 3 Haiku | $0.5 | $0.625 | **$1.125** | â­â­â­â­ |
| DeepSeek V3 | $0.54 | $0.55 | **$1.09** | â­â­â­â­ |
| GPT-4o-mini | $0.3 | $0.3 | **$0.6** | â­â­â­ |

## äº”ã€æœ€ç»ˆæ¨èæ–¹æ¡ˆ

### ğŸ¯ æ¨èç»„åˆï¼šClaude 3.5 Sonnet + GPT-4o-mini

```python
class StoryLLM:
    def __init__(self):
        self.story_model = "claude-3-5-sonnet"  # ç”Ÿæˆå‰§æƒ…
        self.judge_model = "gpt-4o-mini"        # åˆ¤å®šé€»è¾‘
    
    async def generate_story(self, context, user_input):
        """ä½¿ç”¨ Claude ç”Ÿæˆé«˜è´¨é‡å‰§æƒ…"""
        return await anthropic_client.messages.create(
            model=self.story_model,
            max_tokens=1024,
            messages=[
                {"role": "user", "content": f"{context}\n\n{user_input}"}
            ]
        )
    
    async def judge_progress(self, state, story):
        """ä½¿ç”¨ GPT-4o-mini åšåˆ¤å®šï¼ˆä¾¿å®œï¼‰"""
        return await openai_client.chat.completions.create(
            model=self.judge_model,
            messages=[
                {"role": "user", "content": f"åˆ¤æ–­å‰§æƒ…è¿›å±•ï¼š{story}"}
            ]
        )
```

**æˆæœ¬ä¼°ç®—**ï¼ˆ1000 æ¬¡å¯¹è¯ï¼‰ï¼š
- æ•…äº‹ç”Ÿæˆï¼š$7.5ï¼ˆClaudeï¼‰
- åˆ¤å®šé€»è¾‘ï¼š$0.3ï¼ˆGPT-4o-miniï¼‰
- **æ€»è®¡ï¼š$7.8**ï¼ˆç›¸æ¯”å…¨ç”¨ Claude èŠ‚çœ 42%ï¼‰

### ğŸ‡¨ğŸ‡³ å›½å†…æ–¹æ¡ˆï¼šDeepSeek V3 + Qwen2.5

```python
class ChinaLLM:
    def __init__(self):
        self.story_model = "deepseek-chat"
        self.judge_model = "qwen2.5-72b"
    
    # ä½¿ç”¨æ–¹å¼åŒä¸Š
```

**æˆæœ¬ä¼°ç®—**ï¼ˆ1000 æ¬¡å¯¹è¯ï¼‰ï¼š
- **æ€»è®¡ï¼šçº¦ $2**ï¼ˆæä½æˆæœ¬ï¼‰

## å…­ã€API æ¥å…¥æ–¹å¼

### ç»Ÿä¸€æ¥å£ï¼ˆæ¨èä½¿ç”¨ LiteLLMï¼‰

```python
from litellm import completion

# è‡ªåŠ¨é€‚é…ä¸åŒæ¨¡å‹
response = completion(
    model="claude-3-5-sonnet-20241022",  # æˆ– "gpt-4o", "gemini-1.5-pro"
    messages=[{"role": "user", "content": "..."}]
)
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç»Ÿä¸€æ¥å£ï¼Œåˆ‡æ¢æ¨¡å‹åªéœ€æ”¹åç§°
- âœ… è‡ªåŠ¨é‡è¯•ã€è´Ÿè½½å‡è¡¡
- âœ… æˆæœ¬è¿½è¸ª
- âœ… æ”¯æŒ 100+ æ¨¡å‹

## ä¸ƒã€æ€»ç»“

### æœ€ä½³é€‰æ‹©
1. **é¢„ç®—å……è¶³**ï¼šClaude 3.5 Sonnetï¼ˆè´¨é‡æœ€å¥½ï¼‰
2. **å¹³è¡¡æ–¹æ¡ˆ**ï¼šClaude 3.5 Sonnet + GPT-4o-miniï¼ˆæ¨èï¼‰
3. **å›½å†…éƒ¨ç½²**ï¼šDeepSeek V3 + Qwen2.5
4. **æè‡´æ€§ä»·æ¯”**ï¼šClaude 3 Haiku
5. **è‡ªä¸»å¯æ§**ï¼šLlama 3.1 70Bï¼ˆè‡ªéƒ¨ç½²ï¼‰

### å…³é”®å»ºè®®
- âœ… ä½¿ç”¨æ··åˆç­–ç•¥é™ä½æˆæœ¬
- âœ… é‡è¦å‰§æƒ…ç”¨å¥½æ¨¡å‹ï¼Œåˆ¤å®šç”¨ä¾¿å®œæ¨¡å‹
- âœ… æ·»åŠ ç¼“å­˜å‡å°‘é‡å¤è°ƒç”¨
- âœ… ä½¿ç”¨ LiteLLM ç»Ÿä¸€æ¥å£
- âœ… ç›‘æ§æˆæœ¬å’Œè´¨é‡æŒ‡æ ‡
